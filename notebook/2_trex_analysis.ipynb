{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA TensorRT Engines with TREx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert the trained `best_qat.onnx` to `.engine` using `trtexec`\n",
    "\n",
    "- Example command: \n",
    "\n",
    "   ```bash\n",
    "    trtexec --verbose --onnx=../tests/inputs/mobilenet.qat.onnx --saveEngine=../tests/inputs/mobilenet.qat.onnx.engine --exportLayerInfo=../tests/inputs/mobilenet.qat.onnx.engine.graph.json --timingCacheFile=./timing.cache --profilingVerbosity=detailed --best\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeffrymahbuubi/Syringe-Detection/env/tensorrt/bin:/home/jeffrymahbuubi/.vscode-server/bin/5437499feb04f7a586f677b155b039bc2b3669eb/bin/remote-cli:/usr/local/cuda-11.8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Program Files/dotnet/:/mnt/c/Program Files (x86)/Windows Kits/10/Windows Performance Toolkit/:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/Docker/host/bin:/mnt/c/Program Files/usbipd-win/:/mnt/c/Users/11208120/AppData/Local/Microsoft/WindowsApps:/mnt/c/Program Files/JetBrains/PyCharm 2024.1.3/bin:/mnt/c/ProgramData/miniconda3:/mnt/c/ProgramData/miniconda3/Scripts:/mnt/c/ProgramData/miniconda3/Library/bin:/mnt/c/Users/11208120/AppData/Local/Programs/Microsoft VS Code/bin:/snap/bin:/home/jeffrymahbuubi/TensorRT-Installation/Lesson/1_Getting_Started_with_NVIDIA_TensorRT/ngc-cli:/usr/src/tensorrt/bin:/home/jeffrymahbuubi/usr/src/tensorrt/bin:/usr/src/tensorrt/bin:/usr/src/tensorrt/bin\n",
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8601] # trtexec --help\n",
      "=== Model Options ===\n",
      "  --uff=<file>                UFF model\n",
      "  --onnx=<file>               ONNX model\n",
      "  --model=<file>              Caffe model (default = no model, random weights used)\n",
      "  --deploy=<file>             Caffe prototxt file\n",
      "  --output=<name>[,<name>]*   Output names (it can be specified multiple times); at least one output is required for UFF and Caffe\n",
      "  --uffInput=<name>,X,Y,Z     Input blob name and its dimensions (X,Y,Z=C,H,W), it can be specified multiple times; at least one is required for UFF models\n",
      "  --uffNHWC                   Set if inputs are in the NHWC layout instead of NCHW (use X,Y,Z=H,W,C order in --uffInput)\n",
      "\n",
      "=== Build Options ===\n",
      "  --maxBatch                         Set max batch size and build an implicit batch engine (default = same size as --batch)\n",
      "                                     This option should not be used when the input model is ONNX or when dynamic shapes are provided.\n",
      "  --minShapes=spec                   Build with dynamic shapes using a profile with the min shapes provided\n",
      "  --optShapes=spec                   Build with dynamic shapes using a profile with the opt shapes provided\n",
      "  --maxShapes=spec                   Build with dynamic shapes using a profile with the max shapes provided\n",
      "  --minShapesCalib=spec              Calibrate with dynamic shapes using a profile with the min shapes provided\n",
      "  --optShapesCalib=spec              Calibrate with dynamic shapes using a profile with the opt shapes provided\n",
      "  --maxShapesCalib=spec              Calibrate with dynamic shapes using a profile with the max shapes provided\n",
      "                                     Note: All three of min, opt and max shapes must be supplied.\n",
      "                                           However, if only opt shapes is supplied then it will be expanded so\n",
      "                                           that min shapes and max shapes are set to the same values as opt shapes.\n",
      "                                           Input names can be wrapped with escaped single quotes (ex: 'Input:0').\n",
      "                                     Example input shapes spec: input0:1x3x256x256,input1:1x3x128x128\n",
      "                                     Each input shape is supplied as a key-value pair where key is the input name and\n",
      "                                     value is the dimensions (including the batch dimension) to be used for that input.\n",
      "                                     Each key-value pair has the key and value separated using a colon (:).\n",
      "                                     Multiple input shapes can be provided via comma-separated key-value pairs.\n",
      "  --inputIOFormats=spec              Type and format of each of the input tensors (default = all inputs in fp32:chw)\n",
      "                                     See --outputIOFormats help for the grammar of type and format list.\n",
      "                                     Note: If this option is specified, please set comma-separated types and formats for all\n",
      "                                           inputs following the same order as network inputs ID (even if only one input\n",
      "                                           needs specifying IO format) or set the type and format once for broadcasting.\n",
      "  --outputIOFormats=spec             Type and format of each of the output tensors (default = all outputs in fp32:chw)\n",
      "                                     Note: If this option is specified, please set comma-separated types and formats for all\n",
      "                                           outputs following the same order as network outputs ID (even if only one output\n",
      "                                           needs specifying IO format) or set the type and format once for broadcasting.\n",
      "                                     IO Formats: spec  ::= IOfmt[\",\"spec]\n",
      "                                                 IOfmt ::= type:fmt\n",
      "                                               type  ::= \"fp32\"|\"fp16\"|\"int32\"|\"int8\"\n",
      "                                               fmt   ::= (\"chw\"|\"chw2\"|\"chw4\"|\"hwc8\"|\"chw16\"|\"chw32\"|\"dhwc8\"|\n",
      "                                                          \"cdhw32\"|\"hwc\"|\"dla_linear\"|\"dla_hwc4\")[\"+\"fmt]\n",
      "  --workspace=N                      Set workspace size in MiB.\n",
      "  --memPoolSize=poolspec             Specify the size constraints of the designated memory pool(s) in MiB.\n",
      "                                     Note: Also accepts decimal sizes, e.g. 0.25MiB. Will be rounded down to the nearest integer bytes.\n",
      "                                     In particular, for dlaSRAM the bytes will be rounded down to the nearest power of 2.\n",
      "                                   Pool constraint: poolspec ::= poolfmt[\",\"poolspec]\n",
      "                                                      poolfmt ::= pool:sizeInMiB\n",
      "                                                    pool ::= \"workspace\"|\"dlaSRAM\"|\"dlaLocalDRAM\"|\"dlaGlobalDRAM\"\n",
      "  --profilingVerbosity=mode          Specify profiling verbosity. mode ::= layer_names_only|detailed|none (default = layer_names_only)\n",
      "  --minTiming=M                      Set the minimum number of iterations used in kernel selection (default = 1)\n",
      "  --avgTiming=M                      Set the number of times averaged in each iteration for kernel selection (default = 8)\n",
      "  --refit                            Mark the engine as refittable. This will allow the inspection of refittable layers \n",
      "                                     and weights within the engine.\n",
      "  --versionCompatible, --vc          Mark the engine as version compatible. This allows the engine to be used with newer versions\n",
      "                                     of TensorRT on the same host OS, as well as TensorRT's dispatch and lean runtimes.\n",
      "                                     Only supported with explicit batch.\n",
      "  --useRuntime=runtime               TensorRT runtime to execute engine. \"lean\" and \"dispatch\" require loading VC engine and do\n",
      "                                     not support building an engine.\n",
      "                                           runtime::= \"full\"|\"lean\"|\"dispatch\"\n",
      "  --leanDLLPath=<file>               External lean runtime DLL to use in version compatiable mode.\n",
      "  --excludeLeanRuntime               When --versionCompatible is enabled, this flag indicates that the generated engine should\n",
      "                                     not include an embedded lean runtime. If this is set, the user must explicitly specify a\n",
      "                                     valid lean runtime to use when loading the engine.  Only supported with explicit batch\n",
      "                                     and weights within the engine.\n",
      "  --sparsity=spec                    Control sparsity (default = disabled). \n",
      "                                   Sparsity: spec ::= \"disable\", \"enable\", \"force\"\n",
      "                                     Note: Description about each of these options is as below\n",
      "                                           disable = do not enable sparse tactics in the builder (this is the default)\n",
      "                                           enable  = enable sparse tactics in the builder (but these tactics will only be\n",
      "                                                     considered if the weights have the right sparsity pattern)\n",
      "                                           force   = enable sparse tactics in the builder and force-overwrite the weights to have\n",
      "                                                     a sparsity pattern (even if you loaded a model yourself)\n",
      "  --noTF32                           Disable tf32 precision (default is to enable tf32, in addition to fp32)\n",
      "  --fp16                             Enable fp16 precision, in addition to fp32 (default = disabled)\n",
      "  --int8                             Enable int8 precision, in addition to fp32 (default = disabled)\n",
      "  --fp8                              Enable fp8 precision, in addition to fp32 (default = disabled)\n",
      "  --best                             Enable all precisions to achieve the best performance (default = disabled)\n",
      "  --directIO                         Avoid reformatting at network boundaries. (default = disabled)\n",
      "  --precisionConstraints=spec        Control precision constraint setting. (default = none)\n",
      "                                       Precision Constraints: spec ::= \"none\" | \"obey\" | \"prefer\"\n",
      "                                         none = no constraints\n",
      "                                         prefer = meet precision constraints set by --layerPrecisions/--layerOutputTypes if possible\n",
      "                                         obey = meet precision constraints set by --layerPrecisions/--layerOutputTypes or fail\n",
      "                                                otherwise\n",
      "  --layerPrecisions=spec             Control per-layer precision constraints. Effective only when precisionConstraints is set to\n",
      "                                   \"obey\" or \"prefer\". (default = none)\n",
      "                                   The specs are read left-to-right, and later ones override earlier ones. \"*\" can be used as a\n",
      "                                     layerName to specify the default precision for all the unspecified layers.\n",
      "                                   Per-layer precision spec ::= layerPrecision[\",\"spec]\n",
      "                                                       layerPrecision ::= layerName\":\"precision\n",
      "                                                       precision ::= \"fp32\"|\"fp16\"|\"int32\"|\"int8\"\n",
      "  --layerOutputTypes=spec            Control per-layer output type constraints. Effective only when precisionConstraints is set to\n",
      "                                   \"obey\" or \"prefer\". (default = none\n",
      "                                   The specs are read left-to-right, and later ones override earlier ones. \"*\" can be used as a\n",
      "                                     layerName to specify the default precision for all the unspecified layers. If a layer has more than\n",
      "                                   one output, then multiple types separated by \"+\" can be provided for this layer.\n",
      "                                   Per-layer output type spec ::= layerOutputTypes[\",\"spec]\n",
      "                                                         layerOutputTypes ::= layerName\":\"type\n",
      "                                                         type ::= \"fp32\"|\"fp16\"|\"int32\"|\"int8\"[\"+\"type]\n",
      "  --layerDeviceTypes=spec            Specify layer-specific device type.\n",
      "                                     The specs are read left-to-right, and later ones override earlier ones. If a layer does not have\n",
      "                                     a device type specified, the layer will opt for the default device type.\n",
      "                                   Per-layer device type spec ::= layerDeviceTypePair[\",\"spec]\n",
      "                                                         layerDeviceTypePair ::= layerName\":\"deviceType\n",
      "                                                           deviceType ::= \"GPU\"|\"DLA\"\n",
      "  --calib=<file>                     Read INT8 calibration cache file\n",
      "  --safe                             Enable build safety certified engine, if DLA is enable, --buildDLAStandalone will be specified\n",
      "                                     automatically (default = disabled)\n",
      "  --buildDLAStandalone               Enable build DLA standalone loadable which can be loaded by cuDLA, when this option is enabled, \n",
      "                                     --allowGPUFallback is disallowed and --skipInference is enabled by default. Additionally, \n",
      "                                     specifying --inputIOFormats and --outputIOFormats restricts I/O data type and memory layout\n",
      "                                     (default = disabled)\n",
      "  --allowGPUFallback                 When DLA is enabled, allow GPU fallback for unsupported layers (default = disabled)\n",
      "  --consistency                      Perform consistency checking on safety certified engine\n",
      "  --restricted                       Enable safety scope checking with kSAFETY_SCOPE build flag\n",
      "  --saveEngine=<file>                Save the serialized engine\n",
      "  --loadEngine=<file>                Load a serialized engine\n",
      "  --tacticSources=tactics            Specify the tactics to be used by adding (+) or removing (-) tactics from the default \n",
      "                                     tactic sources (default = all available tactics).\n",
      "                                     Note: Currently only cuDNN, cuBLAS, cuBLAS-LT, and edge mask convolutions are listed as optional\n",
      "                                           tactics.\n",
      "                                   Tactic Sources: tactics ::= [\",\"tactic]\n",
      "                                                     tactic  ::= (+|-)lib\n",
      "                                                   lib     ::= \"CUBLAS\"|\"CUBLAS_LT\"|\"CUDNN\"|\"EDGE_MASK_CONVOLUTIONS\"\n",
      "                                                               |\"JIT_CONVOLUTIONS\"\n",
      "                                     For example, to disable cudnn and enable cublas: --tacticSources=-CUDNN,+CUBLAS\n",
      "  --noBuilderCache                   Disable timing cache in builder (default is to enable timing cache)\n",
      "  --heuristic                        Enable tactic selection heuristic in builder (default is to disable the heuristic)\n",
      "  --timingCacheFile=<file>           Save/load the serialized global timing cache\n",
      "  --preview=features                 Specify preview feature to be used by adding (+) or removing (-) preview features from the default\n",
      "                                   Preview Features: features ::= [\",\"feature]\n",
      "                                                       feature  ::= (+|-)flag\n",
      "                                                     flag     ::= \"fasterDynamicShapes0805\"\n",
      "                                                                  |\"disableExternalTacticSourcesForCore0805\"\n",
      "                                                                  |\"profileSharing0806\"\n",
      "  --builderOptimizationLevel         Set the builder optimization level. (default is 3)\n",
      "                                     Higher level allows TensorRT to spend more building time for more optimization options.\n",
      "                                     Valid values include integers from 0 to the maximum optimization level, which is currently 5.\n",
      "  --hardwareCompatibilityLevel=mode  Make the engine file compatible with other GPU architectures. (default = none)\n",
      "                                   Hardware Compatibility Level: mode ::= \"none\" | \"ampere+\"\n",
      "                                         none = no compatibility\n",
      "                                         ampere+ = compatible with Ampere and newer GPUs\n",
      "  --tempdir=<dir>                    Overrides the default temporary directory TensorRT will use when creating temporary files.\n",
      "                                     See IRuntime::setTemporaryDirectory API documentation for more information.\n",
      "  --tempfileControls=controls        Controls what TensorRT is allowed to use when creating temporary executable files.\n",
      "                                     Should be a comma-separated list with entries in the format (in_memory|temporary):(allow|deny).\n",
      "                                     in_memory: Controls whether TensorRT is allowed to create temporary in-memory executable files.\n",
      "                                     temporary: Controls whether TensorRT is allowed to create temporary executable files in the\n",
      "                                                filesystem (in the directory given by --tempdir).\n",
      "                                     For example, to allow in-memory files and disallow temporary files:\n",
      "                                         --tempfileControls=in_memory:allow,temporary:deny\n",
      "                                   If a flag is unspecified, the default behavior is \"allow\".\n",
      "  --maxAuxStreams=N                  Set maximum number of auxiliary streams per inference stream that TRT is allowed to use to run \n",
      "                                     kernels in parallel if the network contains ops that can run in parallel, with the cost of more \n",
      "                                     memory usage. Set this to 0 for optimal memory usage. (default = using heuristics)\n",
      "\n",
      "=== Inference Options ===\n",
      "  --batch=N                   Set batch size for implicit batch engines (default = 1)\n",
      "                              This option should not be used when the engine is built from an ONNX model or when dynamic\n",
      "                              shapes are provided when the engine is built.\n",
      "  --shapes=spec               Set input shapes for dynamic shapes inference inputs.\n",
      "                              Note: Input names can be wrapped with escaped single quotes (ex: 'Input:0').\n",
      "                              Example input shapes spec: input0:1x3x256x256, input1:1x3x128x128\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs.\n",
      "  --loadInputs=spec           Load input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: 'Input:0')\n",
      "                            Input values spec ::= Ival[\",\"spec]\n",
      "                                         Ival ::= name\":\"file\n",
      "  --iterations=N              Run at least N inference iterations (default = 10)\n",
      "  --warmUp=N                  Run for N milliseconds to warmup before measuring performance (default = 200)\n",
      "  --duration=N                Run performance measurements for at least N seconds wallclock time (default = 3)\n",
      "                              If -1 is specified, inference will keep running unless stopped manually\n",
      "  --sleepTime=N               Delay inference start with a gap of N milliseconds between launch and compute (default = 0)\n",
      "  --idleTime=N                Sleep N milliseconds between two continuous iterations(default = 0)\n",
      "  --infStreams=N              Instantiate N engines to run inference concurrently (default = 1)\n",
      "  --exposeDMA                 Serialize DMA transfers to and from device (default = disabled).\n",
      "  --noDataTransfers           Disable DMA transfers to and from device (default = enabled).\n",
      "  --useManagedMemory          Use managed memory instead of separate host and device allocations (default = disabled).\n",
      "  --useSpinWait               Actively synchronize on GPU events. This option may decrease synchronization time but increase CPU usage and power (default = disabled)\n",
      "  --threads                   Enable multithreading to drive engines with independent threads or speed up refitting (default = disabled) \n",
      "  --useCudaGraph              Use CUDA graph to capture engine execution and then launch inference (default = disabled).\n",
      "                              This flag may be ignored if the graph capture fails.\n",
      "  --timeDeserialize           Time the amount of time it takes to deserialize the network and exit.\n",
      "  --timeRefit                 Time the amount of time it takes to refit the engine before inference.\n",
      "  --separateProfileRun        Do not attach the profiler in the benchmark run; if profiling is enabled, a second profile run will be executed (default = disabled)\n",
      "  --skipInference             Exit after the engine has been built and skip inference perf measurement (default = disabled)\n",
      "  --persistentCacheRatio      Set the persistentCacheLimit in ratio, 0.5 represent half of max persistent L2 size (default = 0)\n",
      "\n",
      "=== Build and Inference Batch Options ===\n",
      "                              When using implicit batch, the max batch size of the engine, if not given, \n",
      "                              is set to the inference batch size;\n",
      "                              when using explicit batch, if shapes are specified only for inference, they \n",
      "                              will be used also as min/opt/max in the build profile; if shapes are \n",
      "                              specified only for the build, the opt shapes will be used also for inference;\n",
      "                              if both are specified, they must be compatible; and if explicit batch is \n",
      "                              enabled but neither is specified, the model must provide complete static\n",
      "                              dimensions, including batch size, for all inputs\n",
      "                              Using ONNX models automatically forces explicit batch.\n",
      "\n",
      "=== Reporting Options ===\n",
      "  --verbose                   Use verbose logging (default = false)\n",
      "  --avgRuns=N                 Report performance measurements averaged over N consecutive iterations (default = 10)\n",
      "  --percentile=P1,P2,P3,...   Report performance for the P1,P2,P3,... percentages (0<=P_i<=100, 0 representing max perf, and 100 representing min perf; (default = 90,95,99%)\n",
      "  --dumpRefit                 Print the refittable layers and weights from a refittable engine\n",
      "  --dumpOutput                Print the output tensor(s) of the last inference iteration (default = disabled)\n",
      "  --dumpRawBindingsToFile     Print the input/output tensor(s) of the last inference iteration to file(default = disabled)\n",
      "  --dumpProfile               Print profile information per layer (default = disabled)\n",
      "  --dumpLayerInfo             Print layer information of the engine to console (default = disabled)\n",
      "  --exportTimes=<file>        Write the timing results in a json file (default = disabled)\n",
      "  --exportOutput=<file>       Write the output tensors to a json file (default = disabled)\n",
      "  --exportProfile=<file>      Write the profile information per layer in a json file (default = disabled)\n",
      "  --exportLayerInfo=<file>    Write the layer information of the engine in a json file (default = disabled)\n",
      "\n",
      "=== System Options ===\n",
      "  --device=N                  Select cuda device N (default = 0)\n",
      "  --useDLACore=N              Select DLA core N for layers that support DLA (default = none)\n",
      "  --staticPlugins             Plugin library (.so) to load statically (can be specified multiple times)\n",
      "  --dynamicPlugins            Plugin library (.so) to load dynamically and may be serialized with the engine if they are included in --setPluginsToSerialize (can be specified multiple times)\n",
      "  --setPluginsToSerialize     Plugin library (.so) to be serialized with the engine (can be specified multiple times)\n",
      "  --ignoreParsedPluginLibs    By default, when building a version-compatible engine, plugin libraries specified by the ONNX parser \n",
      "                              are implicitly serialized with the engine (unless --excludeLeanRuntime is specified) and loaded dynamically. \n",
      "                              Enable this flag to ignore these plugin libraries instead.\n",
      "\n",
      "=== Help ===\n",
      "  --help, -h                  Print this message\n"
     ]
    }
   ],
   "source": [
    "# Add the directory containing trtexec to the PATH\n",
    "os.environ['PATH'] += os.pathsep + '/usr/src/tensorrt/bin'\n",
    "\n",
    "# Verify that trtexec is in the PATH\n",
    "!echo $PATH\n",
    "\n",
    "# Test if trtexec is accessible\n",
    "!trtexec --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the engine: /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine\n",
      "trtexec --verbose --onnx=/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/best_qat.onnx --saveEngine=/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine --exportLayerInfo=/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine.graph.json --timingCacheFile=./timing.cache --profilingVerbosity=detailed --best\n",
      "\n",
      "Successfully built the engine.\n",
      "\n",
      "Engine building metadata: generated output file /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine.build.metadata.json\n",
      "Profiling the engine: /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine\n",
      "trtexec --verbose --noDataTransfers --useCudaGraph --separateProfileRun --useSpinWait --loadEngine=/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine --exportTimes=/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine.timing.json --exportProfile=/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine.profile.json --exportLayerInfo=/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine.graph.json --timingCacheFile=./timing.cache --profilingVerbosity=detailed --best\n",
      "\n",
      "Successfully profiled the engine.\n",
      "\n",
      "Profiling metadata: generated output file /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine.profile.metadata.json\n",
      "Generating graph diagram: /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine.graph.json\n",
      "Created file:///home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt/best_qat.onnx.engine.graph.json.svg\n",
      "Artifcats directory: /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt\n"
     ]
    }
   ],
   "source": [
    "# Run the trtexec command to generate the .engine and .json files\n",
    "# !trtexec --verbose --onnx=./run/detect/QAT_b8_e200_q40_relu_rect/weights/best_qat.onnx --saveEngine=./run/detect/QAT_b8_e200_q40_relu_rect/weights/best_qat.engine --int8 --best --device=0\n",
    "\n",
    "# Using `process_engine.py\n",
    "!python /home/jeffrymahbuubi/Syringe-Detection/compression/TensorRT/tools/experimental/trt-engine-explorer/utils/process_engine.py /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/best_qat.onnx /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b8_e200_q40_relu_rect/weights/tensorrt best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load `JSON` Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import trex\n",
    "import trex.notebook\n",
    "import trex.plotting\n",
    "import trex.graphing\n",
    "import trex.df_preprocessing\n",
    "\n",
    "# Configure a wider output (for the wide graphs)\n",
    "trex.notebook.set_wide_display()\n",
    "\n",
    "# Choose an engine file to load. This notebook assumes that you've saved the engine to the following paths.\n",
    "engine_name = \"./run/detect/QAT_b16_e100_silu/weights/tensorrt/best_qat.onnx.engine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an `EnginePlan` instance and start exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for best_qat.onnx.engine.graph.json:\n",
      "\n",
      "Model:\n",
      "\tInputs: images: [1, 3, 640, 640]xFP32 NCHW\n",
      "\tOutputs: pred: [1, 80, 8400]xFP32 NCHW\n",
      "\tAverage time: 7.369 ms\n",
      "\tLayers: 137\n",
      "\tWeights: 3.0 MB\n",
      "\tActivations: 100.5 MB\n",
      "Device Properties:\n",
      "\tSelected Device: NVIDIA GeForce RTX 4070 SUPER\n",
      "\tCompute Capability: 8.9\n",
      "\tSMs: 56.0\n",
      "\tDevice Global Memory: 12281 MiB\n",
      "\tShared Memory per SM: 100 KiB\n",
      "\tMemory Bus Width: 192.0\n",
      "\tApplication Compute Clock Rate: 2.475 GHz\n",
      "\tApplication Memory Clock Rate: 10.501 GHz\n",
      "Builder Configuration:\n",
      "Performance Summary:\n",
      "\tThroughput: 1996.31\n",
      "\tLatency: [0.429932, 4.02026, 0.484867, 0.432129, 0.59903, 0.807861, 1.10229]\n",
      "\tEnqueue Time: [0.0185852, 4.30615, 0.119042, 0.041748, 0.487671, 0.552246, 0.829834]\n",
      "\tH2D Latency: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\tGPU Compute Time: [0.429932, 4.02026, 0.484867, 0.432129, 0.59903, 0.807861, 1.10229]\n",
      "\tD2H Latency: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\tTotal Host Walltime: 3.00154\n",
      "\tTotal GPU Compute Time: 2.90532\n"
     ]
    }
   ],
   "source": [
    "assert engine_name is not None\n",
    "plan = trex.EnginePlan(f'{engine_name}.graph.json', f'{engine_name}.profile.json', f'{engine_name}.profile.metadata.json')\n",
    "\n",
    "print(f\"Summary for {plan.name}:\\n\")\n",
    "plan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Layers Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>latency.pct_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FP16</td>\n",
       "      <td>6.623924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FP32</td>\n",
       "      <td>8.014247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INT8</td>\n",
       "      <td>85.361835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  precision  latency.pct_time\n",
       "0      FP16          6.623924\n",
       "1      FP32          8.014247\n",
       "2      INT8         85.361835"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "domain": {
          "x": [
           0,
           0.45
          ],
          "y": [
           0,
           1
          ]
         },
         "hole": 0.4,
         "hoverinfo": "label+percent",
         "labels": [
          "FP16",
          "FP32",
          "INT8"
         ],
         "marker": {
          "colors": [
           "orange",
           "red",
           "#76b900"
          ],
          "line": {
           "color": "#76b900",
           "width": 1
          }
         },
         "name": "Layer Count By Precision",
         "textfont": {
          "size": 20
         },
         "textinfo": "value",
         "textposition": "inside",
         "texttemplate": "%{value}",
         "type": "pie",
         "values": [
          9,
          11,
          117
         ]
        },
        {
         "domain": {
          "x": [
           0.55,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "hole": 0.4,
         "hoverinfo": "label+percent",
         "labels": [
          "FP16",
          "FP32",
          "INT8"
         ],
         "marker": {
          "colors": [
           "orange",
           "red",
           "#76b900"
          ],
          "line": {
           "color": "#76b900",
           "width": 1
          }
         },
         "name": "% Latency Budget By Precision",
         "textfont": {
          "size": 20
         },
         "textinfo": "value",
         "textposition": "inside",
         "texttemplate": "%{value:.1f}",
         "type": "pie",
         "values": [
          6.623924000000001,
          8.014247000000001,
          85.361835
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Layer Count By Precision",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "% Latency Budget By Precision",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "font": {
         "size": 20
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Precision Statistics",
         "x": 0.5
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"2b28924b-32bc-48f8-b516-5b049368e318\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2b28924b-32bc-48f8-b516-5b049368e318\")) {                    Plotly.newPlot(                        \"2b28924b-32bc-48f8-b516-5b049368e318\",                        [{\"labels\":[\"FP16\",\"FP32\",\"INT8\"],\"marker\":{\"colors\":[\"orange\",\"red\",\"#76b900\"],\"line\":{\"color\":\"#76b900\",\"width\":1}},\"name\":\"Layer Count By Precision\",\"texttemplate\":\"%{value}\",\"values\":[9,11,117],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,1.0]},\"textfont\":{\"size\":20},\"hole\":0.4,\"hoverinfo\":\"label+percent\",\"textinfo\":\"value\",\"textposition\":\"inside\"},{\"labels\":[\"FP16\",\"FP32\",\"INT8\"],\"marker\":{\"colors\":[\"orange\",\"red\",\"#76b900\"],\"line\":{\"color\":\"#76b900\",\"width\":1}},\"name\":\"% Latency Budget By Precision\",\"texttemplate\":\"%{value:.1f}\",\"values\":[6.623924000000001,8.014247000000001,85.361835],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.0,1.0]},\"textfont\":{\"size\":20},\"hole\":0.4,\"hoverinfo\":\"label+percent\",\"textinfo\":\"value\",\"textposition\":\"inside\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Layer Count By Precision\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"% Latency Budget By Precision\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Precision Statistics\",\"x\":0.5},\"font\":{\"size\":20}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2b28924b-32bc-48f8-b516-5b049368e318');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "charts = []\n",
    "layer_precisions = trex.group_count(plan.df, 'precision')\n",
    "charts.append((layer_precisions, 'Layer Count By Precision', 'count', 'precision'))\n",
    "\n",
    "layers_time_pct_by_precision = trex.group_sum_attr(plan.df, grouping_attr='precision', reduced_attr='latency.pct_time')\n",
    "display(layers_time_pct_by_precision)\n",
    "\n",
    "charts.append((layers_time_pct_by_precision, '% Latency Budget By Precision', 'latency.pct_time', 'precision'))\n",
    "trex.plotting.plotly_pie2(\"Precision Statistics\", charts, colormap=trex.colors.precision_colormap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Graph Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file:///home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_silu/weights/tensorrt/best_qat.onnx.engine.svg\n",
      "Created file:///home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_silu/weights/tensorrt/best_qat.onnx.engine.png\n"
     ]
    }
   ],
   "source": [
    "formatter = trex.graphing.layer_type_formatter if True else trex.graphing.precision_formatter\n",
    "graph = trex.graphing.to_dot(plan, formatter)\n",
    "svg_name = trex.graphing.render_dot(graph, engine_name, 'svg')\n",
    "png_name = trex.graphing.render_dot(graph, engine_name, 'png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from memory_profiler import memory_usage\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# Determine if a GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "source = 'https://ultralytics.com/images/bus.jpg'\n",
    "\n",
    "# Function to measure memory and run inference\n",
    "def measure_memory_and_time(model_path, source):\n",
    "    start_time = time.time()\n",
    "    model = YOLO(model_path, task=\"detect\")\n",
    "    results = model.predict(source, device=device, save=True)\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    print(f\"Inference Time: {inference_time} seconds\")\n",
    "    return results, inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Inference and Memory Profiling using PyTorch and ONNX\n",
    "\n",
    "| Model                             | File            | Inference Time (seconds) |\n",
    "|-----------------------------------|-----------------|--------------------------|\n",
    "| Model Original + QAT in ONNX      | `best_qat.onnx` | 2.3577654361724854       |\n",
    "| Model Original + QAT in PyTorch   | `best_qat.pt`   | 0.5823156833648682       |\n",
    "| Model Original in PyTorch         | `best.pt`       | 0.5680086612701416       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory Usage: [1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375, 1886.80859375] MB\n",
      "Loading /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_relu/weights/best_qat.engine for TensorRT inference...\n",
      "[07/05/2024-00:27:20] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[07/05/2024-00:27:20] [TRT] [I] Loaded engine size: 7 MiB\n",
      "[07/05/2024-00:27:20] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +4, now: CPU 1, GPU 28 (MiB)\n",
      "[07/05/2024-00:27:20] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +8, now: CPU 1, GPU 36 (MiB)\n",
      "WARNING  Metadata not found for 'model=/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_relu/weights/best_qat.engine'\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/bus.jpg: 640x640 4 class0s, 1 class5, 6.1ms\n",
      "Speed: 8.1ms preprocess, 6.1ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/predict6\u001b[0m\n",
      "Inference Time: 0.21329689025878906 seconds\n",
      "Peak Memory Usage: 3288.87890625 MB\n",
      "Average Memory Usage: 3088.5479910714284 MB\n"
     ]
    }
   ],
   "source": [
    "# Path to the model\n",
    "model_path = '/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_relu/weights/best_qat.engine'\n",
    "# model_path = '/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_silu/weights/best_qat.engine'\n",
    "# model_path = \"/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_relu/weights/best_qat.onnx\"\n",
    "# model_path = \"/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_silu/weights/best.pt\"\n",
    "# model_path = \"/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_silu/weights/best_qat.pt\"\n",
    "\n",
    "# Measure initial memory usage\n",
    "initial_memory = memory_usage(-1, interval=0.05, timeout=1)\n",
    "print(f\"Initial Memory Usage: {initial_memory} MB\")\n",
    "\n",
    "# Run inference and measure memory usage and inference time\n",
    "results, inference_time = None, None\n",
    "mem_usage = memory_usage(\n",
    "    (measure_memory_and_time, (model_path, source)),\n",
    "    interval=0.05,\n",
    "    include_children=True,\n",
    "    max_usage=False,\n",
    "    retval=True,\n",
    ")\n",
    "\n",
    "# Extract results and memory usage\n",
    "if isinstance(mem_usage, tuple):\n",
    "    mem_usage, (results, inference_time) = mem_usage\n",
    "\n",
    "# Measure final peak memory usage\n",
    "final_peak_memory = max(mem_usage)\n",
    "\n",
    "# Calculate average memory usage\n",
    "average_memory = sum(mem_usage) / len(mem_usage)\n",
    "\n",
    "print(f\"Peak Memory Usage: {final_peak_memory} MB\")\n",
    "print(f\"Average Memory Usage: {average_memory} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Inference using `.engine` Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-qat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
