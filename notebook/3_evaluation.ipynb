{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation YoloV8n Ultralytics QAT (Quantization Aware Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.checks import cuda_is_available\n",
    "from ultralytics.utils.benchmarks import benchmark\n",
    "from pathlib import Path\n",
    "\n",
    "device = 'cuda' if cuda_is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete ✅ (8 CPUs, 15.6 GB RAM, 98.6/250.9 GB disk)\n",
      "\n",
      "Benchmarks complete for best.pt on coco128.yaml at imgsz=640 (511.04s)\n",
      "                   Format Status❔  Size (MB)  metrics/mAP50-95(B)  Inference time (ms/im)     FPS\n",
      "0                 PyTorch       ✅        6.2               0.7156                   48.19   20.75\n",
      "1             TorchScript       ✅       12.5               0.7134                   11.54   86.68\n",
      "2                    ONNX       ✅       12.2               0.7133                   17.15   58.31\n",
      "3                OpenVINO       ❌        0.0                  NaN                     NaN     NaN\n",
      "4                TensorRT       ✅       19.3               0.7132                    5.96  167.83\n",
      "5                  CoreML       ❌        0.0                  NaN                     NaN     NaN\n",
      "6   TensorFlow SavedModel       ✅       30.6               0.7134                   33.36   29.98\n",
      "7     TensorFlow GraphDef       ✅       12.3               0.7134                   32.00   31.25\n",
      "8         TensorFlow Lite       ❌        0.0                  NaN                     NaN     NaN\n",
      "9     TensorFlow Edge TPU       ❌        0.0                  NaN                     NaN     NaN\n",
      "10          TensorFlow.js       ❌        0.0                  NaN                     NaN     NaN\n",
      "11           PaddlePaddle       ✅       24.5               0.7133                  384.52    2.60\n",
      "12                   NCNN       ✅       12.2               0.7133                  165.77    6.03\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Format</th>\n",
       "      <th>Status❔</th>\n",
       "      <th>Size (MB)</th>\n",
       "      <th>metrics/mAP50-95(B)</th>\n",
       "      <th>Inference time (ms/im)</th>\n",
       "      <th>FPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PyTorch</td>\n",
       "      <td>✅</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>48.19</td>\n",
       "      <td>20.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TorchScript</td>\n",
       "      <td>✅</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.7134</td>\n",
       "      <td>11.54</td>\n",
       "      <td>86.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONNX</td>\n",
       "      <td>✅</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.7133</td>\n",
       "      <td>17.15</td>\n",
       "      <td>58.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OpenVINO</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TensorRT</td>\n",
       "      <td>✅</td>\n",
       "      <td>19.3</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>5.96</td>\n",
       "      <td>167.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CoreML</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TensorFlow SavedModel</td>\n",
       "      <td>✅</td>\n",
       "      <td>30.6</td>\n",
       "      <td>0.7134</td>\n",
       "      <td>33.36</td>\n",
       "      <td>29.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TensorFlow GraphDef</td>\n",
       "      <td>✅</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.7134</td>\n",
       "      <td>32.00</td>\n",
       "      <td>31.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TensorFlow Lite</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TensorFlow Edge TPU</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TensorFlow.js</td>\n",
       "      <td>❌</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PaddlePaddle</td>\n",
       "      <td>✅</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.7133</td>\n",
       "      <td>384.52</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NCNN</td>\n",
       "      <td>✅</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0.7133</td>\n",
       "      <td>165.77</td>\n",
       "      <td>6.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Format Status❔  Size (MB)  metrics/mAP50-95(B)  Inference time (ms/im)     FPS\n",
       "0                 PyTorch       ✅        6.2               0.7156                   48.19   20.75\n",
       "1             TorchScript       ✅       12.5               0.7134                   11.54   86.68\n",
       "2                    ONNX       ✅       12.2               0.7133                   17.15   58.31\n",
       "3                OpenVINO       ❌        0.0                  NaN                     NaN     NaN\n",
       "4                TensorRT       ✅       19.3               0.7132                    5.96  167.83\n",
       "5                  CoreML       ❌        0.0                  NaN                     NaN     NaN\n",
       "6   TensorFlow SavedModel       ✅       30.6               0.7134                   33.36   29.98\n",
       "7     TensorFlow GraphDef       ✅       12.3               0.7134                   32.00   31.25\n",
       "8         TensorFlow Lite       ❌        0.0                  NaN                     NaN     NaN\n",
       "9     TensorFlow Edge TPU       ❌        0.0                  NaN                     NaN     NaN\n",
       "10          TensorFlow.js       ❌        0.0                  NaN                     NaN     NaN\n",
       "11           PaddlePaddle       ✅       24.5               0.7133                  384.52    2.60\n",
       "12                   NCNN       ✅       12.2               0.7133                  165.77    6.03"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('/home/jeffrymahbuubi/Syringe-Detection/compression/ultralytics-qat-jeffry/run/detect/QAT_b16_e100_silu/weights/best.pt')\n",
    "\n",
    "model = YOLO(path, task='detect')\n",
    "benchmark(model=model, data='coco128.yaml', imgsz=640, half=False, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_path(model_name, load_weights=True):\n",
    "    \"\"\"\n",
    "    Get the base path for the model directory and weights file.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model.\n",
    "        load_weights (bool, optional): Whether to return the weights path. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Directory path and optionally weights path.\n",
    "    \"\"\"\n",
    "    directory = f'../run/detect/{model_name}/'\n",
    "    weights = f'{directory}weights/best.pt' if load_weights else None\n",
    "\n",
    "    return (directory, weights) if load_weights else directory\n",
    "\n",
    "def benchmark_inference_speed(model, img, imgsz=640, device='cpu', half=False, nwarmup=50, num_runs=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Benchmark the inference speed of a model by averaging over multiple runs.\n",
    "\n",
    "    Args:\n",
    "        model (YOLO): The YOLO model to benchmark.\n",
    "        img (str): The image URL or path to use for inference.\n",
    "        imgsz (int, optional): The image size for the benchmark. Default is 640.\n",
    "        device (str, optional): The device to run the benchmark on, either 'cpu' or 'cuda'. Default is 'cpu'.\n",
    "        half (bool, optional): Use half-precision for the model if True. Default is False.\n",
    "        nwarmup (int, optional): The number of warmup runs. Default is 50.\n",
    "        num_runs (int, optional): The number of runs to average the inference time. Default is 1000.\n",
    "        verbose (bool, optional): Print detailed logs if True. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        float: The average inference time in milliseconds.\n",
    "        float: The average FPS (Frames Per Second).\n",
    "    \"\"\"\n",
    "    # List to store inference times\n",
    "    inference_times = []\n",
    "\n",
    "    # Warmup runs\n",
    "    for _ in range(nwarmup):\n",
    "        _ = model.predict(img, imgsz=imgsz, device=device, half=half, verbose=verbose)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Timed runs\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        results = model.predict(img, imgsz=imgsz, device=device, half=half, verbose=verbose)\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate inference time and append to the list\n",
    "        inference_time = results[0].speed[\"inference\"]\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "    # Calculate average inference time and FPS\n",
    "    average_inference_time = np.mean(inference_times)\n",
    "    average_fps = 1000 / average_inference_time\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Average Inference Time: {average_inference_time:.2f} ms\")\n",
    "        print(f\"Average FPS: {average_fps:.2f}\")\n",
    "\n",
    "    return average_inference_time, average_fps\n",
    "\n",
    "def load_benchmark_results(models):\n",
    "    \"\"\"\n",
    "    Load and display benchmark results for each model in the models dictionary.\n",
    "\n",
    "    Args:\n",
    "        models (dict): Dictionary containing model keys and names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined benchmark results DataFrame.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    for model_key, model_name in models.items():\n",
    "        save_dir = get_base_path(model_name, load_weights=False)\n",
    "        csv_path = f'{save_dir}/benchmark_results.csv'\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df['Model Key'] = model_key  # Add a column to identify which model the results belong to\n",
    "            all_results.append(df)\n",
    "            # Add a separator row\n",
    "            separator_row = pd.DataFrame([['-'*10, '-'*10, '-'*10, '-'*10]], columns=df.columns)\n",
    "            all_results.append(separator_row)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {csv_path}: {e}\")\n",
    "\n",
    "    if all_results:\n",
    "        combined_df = pd.concat(all_results, ignore_index=True)\n",
    "        print(combined_df)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No benchmark results found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation to find mAP50-95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction to find _Latency(ms)_ and _FPS_ \n",
    "\n",
    "**Average Inference Speed**\n",
    "\n",
    "The average inference speed (in milliseconds) is calculated by averaging the individual inference times.\n",
    "\n",
    "$$\\text{Average Inference Time (ms)} = \\frac{1}{N} \\sum_{i=1}^{N} t_i$$\n",
    "\n",
    "where:\n",
    "- \\( N \\) is the number of inference runs.\n",
    "- \\( t_i \\) is the inference time for the \\( i \\)-th run.\n",
    "\n",
    "**Frames Per Second (FPS)**\n",
    "\n",
    "The FPS is calculated as the reciprocal of the average inference time (in seconds).\n",
    "\n",
    "$$\\text{FPS} = \\frac{1000}{\\text{Average Inference Time (ms)}}$$\n",
    "\n",
    "Putting it together, we can calculate the FPS as:\n",
    "\n",
    "$$\\text{FPS} = \\frac{1000}{\\frac{1}{N} \\sum_{i=1}^{N} t_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store model formats and their corresponding weights\n",
    "models = {\n",
    "    'PyTorch': 'best.pt',\n",
    "    'PyTorchQAT': 'best_qat.pt',\n",
    "    'Engine': 'best_qat.engine'\n",
    "}\n",
    "\n",
    "# Base model name\n",
    "# MODEL_NAME = 'QAT_b16_e200_q20_silu_rect_rotate_detection'\n",
    "# MODEL_NAME = 'QAT_b16_e100_q40_silu_rect_syringe_detection'\n",
    "# MODEL_NAME = 'QAT_b16_e100_q40_silu_rect_rubber_detection'\n",
    "MODEL_NAME = 'QAT_b16_e100_q40_silu_rect_line_detection_50cc'\n",
    "\n",
    "images = {\n",
    "    'rotate': './datasets/rotate-detection-qat/valid/images/50cc_46ml_red_v2_8_jpg.rf.a5c8196aa94985df7158469594f775ef.jpg',\n",
    "    'syringe': './datasets/syringe-detection-qat/train/images/50cc_46ml_red_v2_8_jpg.rf.7aa009fd59101708aa31cd85baece847.jpg',\n",
    "    'rubber': './datasets/syringe-detection-qat/valid/images/50cc_46ml_red_v2_8_jpg.rf.d5e5c08395fafd01a273e1eaf70bc29c.jpg',\n",
    "    'line': './datasets/line-detection-qat/cc50/valid/images/50cc_46ml_red_v2_10_jpg.rf.b4451a2f564a61f713934b3838683075.jpg'\n",
    "}\n",
    "\n",
    "PREDICT = 'rotate'\n",
    "PREDICT = 'syringe'\n",
    "PREDICT = 'rubber'\n",
    "PREDICT = 'line'\n",
    "\n",
    "# List to store benchmark results\n",
    "benchmark_results = []\n",
    "\n",
    "# Iterate through the dictionary and benchmark each model\n",
    "for model_name, model_weights in models.items():\n",
    "    print(f\"\\nBenchmarking {model_name} model...\")\n",
    "\n",
    "    # Get the base path for the current model\n",
    "    save_dir, weights = get_base_path(MODEL_NAME, model_weights)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = YOLO(weights)\n",
    "\n",
    "    # Benchmark the model\n",
    "    average_inference_time, average_fps = benchmark_inference_speed(model, img=images[PREDICT], device=device, nwarmup=50, num_runs=1000, verbose=True)\n",
    "\n",
    "    # Append results to the list\n",
    "    benchmark_results.append({\n",
    "        'Model': model_name,\n",
    "        'Average Inference Time (ms)': average_inference_time,\n",
    "        'Average FPS': average_fps\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the benchmark results\n",
    "df = pd.DataFrame(benchmark_results)\n",
    "\n",
    "# Save the DataFrame to a CSV file in the directory\n",
    "csv_path = f'{save_dir}/benchmark_results.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model Average Inference Time (ms) Average FPS            Model Key\n",
      "0      PyTorch                   21.338333   46.864016  ROTATION_MODEL_NAME\n",
      "1   PyTorchQAT                   25.643749   38.995857  ROTATION_MODEL_NAME\n",
      "2       Engine                    9.198055  108.718642  ROTATION_MODEL_NAME\n",
      "3   ----------                  ----------  ----------           ----------\n",
      "4      PyTorch                    21.90941   45.642489   SYRINGE_MODEL_NAME\n",
      "5   PyTorchQAT                   26.837237   37.261659   SYRINGE_MODEL_NAME\n",
      "6       Engine                    9.735532  102.716523   SYRINGE_MODEL_NAME\n",
      "7   ----------                  ----------  ----------           ----------\n",
      "8      PyTorch                   21.154766   47.270673    RUBBER_MODEL_NAME\n",
      "9   PyTorchQAT                    24.30437   41.144865    RUBBER_MODEL_NAME\n",
      "10      Engine                   10.265786   97.410956    RUBBER_MODEL_NAME\n",
      "11  ----------                  ----------  ----------           ----------\n",
      "12     PyTorch                   20.820942   48.028567      LINE_MODEL_NAME\n",
      "13  PyTorchQAT                   23.824453   41.973682      LINE_MODEL_NAME\n",
      "14      Engine                    9.018677  110.881008      LINE_MODEL_NAME\n",
      "15  ----------                  ----------  ----------           ----------\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of model names\n",
    "models = {\n",
    "    'ROTATION_MODEL_NAME': 'QAT_b16_e200_q20_silu_rect_rotate_detection',\n",
    "    'SYRINGE_MODEL_NAME': 'QAT_b16_e100_q40_silu_rect_syringe_detection',\n",
    "    'RUBBER_MODEL_NAME': 'QAT_b16_e100_q40_silu_rect_rubber_detection',\n",
    "    'LINE_MODEL_NAME': 'QAT_b16_e100_q40_silu_rect_line_detection_50cc'\n",
    "}\n",
    "\n",
    "# Load and display benchmark results\n",
    "combined_benchmark_results = load_benchmark_results(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
